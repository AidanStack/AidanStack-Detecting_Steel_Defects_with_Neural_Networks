{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52ff5188",
   "metadata": {},
   "source": [
    "# Binary And Multiclass Classification with Convolutional Neural Networks\n",
    "\n",
    "## Business Overview \n",
    "\n",
    "For the binary classification portion of this notebook, the business use case is identical to the one stated in the artificial neural network notebook. However because convolutional neural networks are designed for image classification tasks, they will likely perform better than the artificial neural networks did. The results of the previous A.N.N.s will serve as a baseline comparison to the results here. \n",
    "\n",
    "The second component of this notebook is to explore the effectiveness of convolutional neural networks when the task is multiclass instead of binary. Since C.N.N.s are designed for image classification, they will hopefully be able to not only determine which images contain defects, but specifically which class of defect is present. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0a9da",
   "metadata": {},
   "source": [
    "## Importing the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b4d269e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing the relevant packages\n",
    "import s3fs\n",
    "\n",
    "import random as rn\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential \n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 6\n",
    "np.random.seed(6)\n",
    "rn.seed(6)\n",
    "tf.random.set_seed(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7374f",
   "metadata": {},
   "source": [
    "## Importing and Reshaping the Image Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb928dc",
   "metadata": {},
   "source": [
    "Since we already preprocessed our images in the last notebook, here we simply import the image arrays. They are currently all 1-Dimensional, so we will have to reshape them for use in our convolutional networks. These arrays will be used for both binary and multiclass classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115a1513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shape of image arrays: (65536,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the image arrays \n",
    "image_arrays = np.load('image_and_label_arrays/image_arrays.npy')\n",
    "\n",
    "# Checking the shape of the first array\n",
    "print(f'Current shape of image arrays: {image_arrays[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92fbbe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolutional_arrays type:      <class 'numpy.ndarray'>\n",
      "Length of convolutional_arrays: 12567\n",
      "Shape of first element:         (256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "# Making a new list to hold our reshaped arrays\n",
    "convolutional_arrays = []\n",
    "\n",
    "# Iterating through our arrays and appended the reshaped result to our list\n",
    "for i in image_arrays:\n",
    "    convolutional_arrays.append(i.reshape(256, 256, 1))\n",
    "\n",
    "# Changing our list into a numpy array\n",
    "convolutional_arrays = np.array(convolutional_arrays)\n",
    "\n",
    "\n",
    "# Checking the type and length of our array\n",
    "print(f'convolutional_arrays type:      {type(convolutional_arrays)}')\n",
    "print(f'Length of convolutional_arrays: {len(convolutional_arrays)}')\n",
    "\n",
    "# checking the shape of the first element\n",
    "print(f'Shape of first element:         {convolutional_arrays[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9bcc8e",
   "metadata": {},
   "source": [
    "The list holding our reshaped input features is now a numpy array, with a length of 12,567, matching the number of training images. Each element in this array represents a training image and has a shape of 256 by 256 by 1. This format, combined with the scaling of each pixel value means our image arrays are ready for use in a convolutional neural network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa33254",
   "metadata": {},
   "source": [
    "# Binary Classification \n",
    "\n",
    "The first task given to the convolutional networks will be binary classification. The performance of the artificial neural networks will be used as a benchmark for comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f2dae",
   "metadata": {},
   "source": [
    "## 1. Loading the Labels\n",
    "Since binary labels were created and saved in the A.N.N. notebook, we can simply load that numpy array for use in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "702fc4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the binary class labels from the last notebook\n",
    "binary_labels = np.load('image_and_label_arrays/label_arrays.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c51ea",
   "metadata": {},
   "source": [
    "## 2. Train Test Splits\n",
    "\n",
    "This step mimics the splits in the A.N.N. notebook. The holdout set will be ignored until final model evaluation, while the training data is split into train and validation sets for model iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a485ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split number 1: This holdout set wont return until the end of the notebook during evaluation\n",
    "X_training, X_hold, y_training, y_hold = train_test_split(convolutional_arrays,\n",
    "                                                          binary_labels,\n",
    "                                                          random_state = seed,\n",
    "                                                          test_size = .1)\n",
    "\n",
    "# Split number 2: Train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_training, \n",
    "                                                  y_training,\n",
    "                                                  random_state = seed,\n",
    "                                                  test_size = .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc47cf7",
   "metadata": {},
   "source": [
    "## 3. Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7d145d",
   "metadata": {},
   "source": [
    "### 3.1 Graphing Function\n",
    "\n",
    "This function is the same as the one in the last notebook, and allows us to easily graph the train and validation performance of our networks over their training epochs with a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac4a6370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to simplify graphing results\n",
    "def graph_results(results):\n",
    "    \n",
    "    '''This function intakes a keras History object and graphs learning curves\n",
    "    using Loss, and then Accuracy for both training and validation. '''\n",
    "    \n",
    "    hist = results.history\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    plt.plot(hist['loss'])\n",
    "    plt.plot(hist['val_loss'])\n",
    "    plt.legend(['Loss', 'Val_Loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    plt.plot(hist['acc'])\n",
    "    plt.plot(hist['val_acc'])\n",
    "    plt.legend(['Train Accuracy', 'Val Accuracy'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871d4570",
   "metadata": {},
   "source": [
    "### 3.1 First Network\n",
    "\n",
    "This network will be as simple as possible, in order to determine whether a simple and less computationally instensive approach to the task is viable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eaff21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv_1 = Sequential()\n",
    "\n",
    "conv_1.add(layers.Conv2D(64, (2, 2), activation='relu', input_shape = (256, 256, 1)))\n",
    "conv_1.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "conv_1.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "conv_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "conv_1.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "conv_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "conv_1.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "conv_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "conv_1.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "conv_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "conv_1.add(layers.Flatten())\n",
    "\n",
    "conv_1.add(layers.Dense(128, activation='relu'))\n",
    "conv_1.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "conv_1.add(layers.Dense(2))\n",
    "\n",
    "conv_1.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['acc'])\n",
    "\n",
    "conv_1.summary()\n",
    "\n",
    "conv_1_results = conv_1.fit(X_train, y_train,\n",
    "                                epochs = 10,\n",
    "                                batch_size = 32,\n",
    "                                validation_data = (X_val, y_val))\n",
    "\n",
    "graph_results(conv_1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b09c388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 255, 255, 64)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 127, 127, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 126, 126, 64)      16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 62, 62, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 30, 30, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 6, 6, 64)          16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 64)          16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 296,770\n",
      "Trainable params: 296,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7917 samples, validate on 3393 samples\n",
      "Epoch 1/10\n",
      " 640/7917 [=>............................] - ETA: 3:53 - loss: 6.4851 - acc: 0.4977"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-86e3dd7e60d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m                                 \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                 \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                                 validation_data = (X_val, y_val))\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mgraph_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_1_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "conv_1 = Sequential()\n",
    "\n",
    "conv_1.add(layers.Conv2D(64, (2, 2), activation='relu', input_shape = (256, 256, 1)))\n",
    "conv_1.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "conv_1.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "conv_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "conv_1.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "conv_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "conv_1.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "conv_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "conv_1.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "conv_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "conv_1.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "conv_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "conv_1.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "conv_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "conv_1.add(layers.Flatten())\n",
    "\n",
    "conv_1.add(layers.Dense(512, activation='relu', kernel_regularizer = l2(0.01)))\n",
    "conv_1.add(layers.Dense(256, activation='relu', kernel_regularizer = l2(0.01)))\n",
    "conv_1.add(layers.Dense(128, activation='relu', kernel_regularizer = l2(0.01)))\n",
    "\n",
    "\n",
    "conv_1.add(layers.Dense(2))\n",
    "\n",
    "conv_1.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['acc'])\n",
    "\n",
    "conv_1.summary()\n",
    "\n",
    "conv_1_results = conv_1.fit(X_train, y_train,\n",
    "                                epochs = 10,\n",
    "                                batch_size = 32,\n",
    "                                validation_data = (X_val, y_val))\n",
    "\n",
    "graph_results(conv_1_results)\n",
    "# Switching to a Covolutional Neural Network to see if it does better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f004df",
   "metadata": {},
   "source": [
    "# Multiclass Classification with Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f88d060",
   "metadata": {},
   "source": [
    "## 1. Creating Multiclass Training Labels\n",
    "\n",
    "Our image importing and proprocessing has already been done in cells above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8c3ccba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0007a71bf.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>18661 28 18863 82 19091 110 19347 110 19603 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a4bcdd.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>37607 3 37858 8 38108 14 38359 20 38610 25 388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000f6bf48.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>131973 1 132228 4 132483 6 132738 8 132993 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0014fce06.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>229501 11 229741 33 229981 55 230221 77 230468...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0025bde0c.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>8458 14 8707 35 8963 48 9219 71 9475 88 9731 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>002af848d.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>290800 6 291055 13 291311 15 291566 18 291822 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>002fc4e19.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>146021 3 146275 10 146529 40 146783 46 147038 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0030401a5.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>186833 1 187089 3 187344 6 187600 7 187855 10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0046839bd.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>152926 1 153180 4 153434 6 153689 8 153943 11 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ImageId  ClassId                                      EncodedPixels\n",
       "0   0002cc93b.jpg        1  29102 12 29346 24 29602 24 29858 24 30114 24 3...\n",
       "1   0007a71bf.jpg        3  18661 28 18863 82 19091 110 19347 110 19603 11...\n",
       "2   000a4bcdd.jpg        1  37607 3 37858 8 38108 14 38359 20 38610 25 388...\n",
       "3   000f6bf48.jpg        4  131973 1 132228 4 132483 6 132738 8 132993 11 ...\n",
       "4   0014fce06.jpg        3  229501 11 229741 33 229981 55 230221 77 230468...\n",
       "5   0025bde0c.jpg        3  8458 14 8707 35 8963 48 9219 71 9475 88 9731 8...\n",
       "7   002af848d.jpg        4  290800 6 291055 13 291311 15 291566 18 291822 ...\n",
       "8   002fc4e19.jpg        1  146021 3 146275 10 146529 40 146783 46 147038 ...\n",
       "10  0030401a5.jpg        4  186833 1 187089 3 187344 6 187600 7 187855 10 ...\n",
       "11  0046839bd.jpg        3  152926 1 153180 4 153434 6 153689 8 153943 11 ..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating an S3 filesystem object\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "# Accessing our csv file from S3\n",
    "csv = fs.open('s3://steel-training-dataset/train.csv')\n",
    "\n",
    "# Tranforming it into a pandas dataframe\n",
    "train_df = pd.read_csv(csv)\n",
    "\n",
    "# Dropping duplicates, making this a multiclass problem instead of multilabel\n",
    "train_df.drop_duplicates(['ImageId'], inplace=True)\n",
    "\n",
    "# Displaying the first ten rows\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "346b2705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0002cc93b.jpg', 1),\n",
       " ('00031f466.jpg', 0),\n",
       " ('000418bfc.jpg', 0),\n",
       " ('000789191.jpg', 0),\n",
       " ('0007a71bf.jpg', 3),\n",
       " ('000a4bcdd.jpg', 1),\n",
       " ('000f6bf48.jpg', 4),\n",
       " ('0014fce06.jpg', 3),\n",
       " ('001982b08.jpg', 0),\n",
       " ('001d1b355.jpg', 0)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a list holding a filepath for every image in our training set \n",
    "path_list = fs.ls('s3://steel-training-dataset/training_images/train_images')\n",
    "\n",
    "# This is a list all of our jpg filenames\n",
    "filenames = [i.split('/')[-1] for i in path_list]\n",
    "\n",
    "# Making a list of zeroes to use as the default label \n",
    "zeroes  = [0 for x in range(len(filenames))]\n",
    "\n",
    "# This dictionary has the jpg names as keys, and zeroes as values \n",
    "label_dict = dict(zip(filenames, zeroes))\n",
    "\n",
    "# Iterating through filenames and \n",
    "for i in filenames:\n",
    "    if i in list(train_df.ImageId):\n",
    "        label_dict[i] = list(train_df.loc[train_df['ImageId'] == i]['ClassId'])[0]\n",
    "\n",
    "# Displaying the first ten elements of label_dict\n",
    "list(label_dict.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1801069b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turning our target labels into an array \n",
    "label_arr = list(label_dict.values())\n",
    "\n",
    "# One hot encoding our binary labels\n",
    "labels_ohe = to_categorical(label_arr)\n",
    "\n",
    "# Sanity check\n",
    "print(len(labels_ohe))\n",
    "\n",
    "# Displaying the first ten rows\n",
    "labels_ohe[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9401c5",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd83a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4ed47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c55f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c2396db",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The deployment of a network capable of multiclass classification would largely be up to the manufacturing engineers, as domain knowledge about the various types of defects would determine how each piece was treated. It is easy to imagine however the various new options that multiclass classification would offer. If certain classes of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69934c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a1f416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
